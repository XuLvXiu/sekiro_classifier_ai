
## 动机与成效

训练一个简单的图片分类模型去挑战弦一郎，这事似乎没有什么太大的意义。

但是，我觉得这是一个起点，有了这套程序框架，后续便可以在此基础上学习其他模型的相关知识。

九月份，经过数十次挑战弦一郎的一阶段，收集了数十万张图片（大部分图片没用到），最终训练出了一个准确率不太高的 resnet18 图片分类模型，

在人工的辅助下（锁定、起身、后撤喝血瓶），白金状态的只狼有一定的机会战胜一阶段的弦一郎，但是打的并不漂亮，水平比人类新手玩家还要略差一些，会错过一些明显的出刀机会。

## 演示视频

三周目白金的狼 再战 稀世强者弦一郎

https://www.bilibili.com/video/BV1rtHxz1E5j/

## 游戏设置

- steam 开始游戏
- 游戏设置：图像设定 -- 屏幕模式设置为窗口，游戏分辨率调整为1280*720
- 游戏设置：按键设置 -- 重置视角/固定目标设置为 q，跳跃键为 f，垫步/识破键为空格，鼠标左键攻击，右键防御。
- 保持游戏窗口在最上层，不要最小化或者被其它的全屏窗口覆盖。

## 训练

- 清除历史数据:
	
`python data_reset.py` 会删除 images 文件夹中全部数据，并删除 labels.csv 文件.

同时也会生成  images/original 目录，用于下一步的数据集收集。

- 收集数据集: 
`python data_collector.py`

按 ] 键开始或暂停收集; 

所谓的收集，就是在打游戏的过程中，定期对游戏屏幕的敌兵区域进行截屏(300x300)，同时记录按下的按键，以 list 的形式保存在内存中。

暂停的时候，内存中的数据集会被附加保存到硬盘文件中。

按 Backspace 键，退出程序

可以预见的是，数据集肯定是不均衡的，在战斗过程中，游戏角色大多处于防御(按下了鼠标右键)或者空闲状态(未按下任何键)，只在极少情况下会攻击（按下鼠标左键）、识破(空格键，用来处理突刺危)、跳跃(f键，用来处理下段的危)。

所以有可能的话，人工对数据集进行一番精修，去掉一些重复的或者意义不大的防御/空闲状态的数据。

当然，不精修的话，后续流程也是可以跑通的。

另外，打法要尽量简单，以一个人类新手玩家的打法来打，以免 AI 学不会。。。

- 训练模型

`python train.py`

首先它会对数据集进行重新采样，随机删除某些分类中的图片以达到相对均衡的状态。

然后切分训练集与测试集并训练一个 resnet18 分类模型

最后在测试集上评估模型的效果。 

在不精修数据集的情况下，11万张原始图片，各分类图片数量如下：

```
0    6750  IDLE
1     432  攻击
2    4026  防御
3      83  识破
```

分类 3 的图片数量极少，也是符合预期的，有时候一阶段打完，都不会出现一次突刺危。


采样后 577 files [153, 163, 178, 83], 四分类模型准确率大概在 47% - 62% 左右，准确率高的模型并不一定更合适去打 boss，反倒是比较保守一直处于防御状态的模型更合适一些，主动出刀进攻的模型死的太快。


各分类准确率：
```

eval accuracy:  {'0': 0.09090909090909091, '1': 0.6595744680851063, '2': 0.7586206896551724, '3': 0.7666666666666667, '4': 0.0}

0 未按下任何键，空闲状态
1 按下鼠标左键，攻击 
2 按下鼠标右键，防御
3 按下空格键，识破
4 按下 f 键，跳跃
```

从准确率上看起来有点糟糕， 但是游戏本身的容错其实很高，AI 也可以玩的下去，而且后面会对 IDLE 进行特殊处理。

## 预测

进入游戏，

在 cmd 窗口中运行：
```
python main.py 
```

等待模型加载完，

按 q 键锁定敌方

按 ] 键, 就会针对敌方的出招自动做出预测动作了。

再次按下 ] 键，会停止预测。

按 Backspace 键，退出程序。

由于打弦一郎的时候，IDLE 状态是没有什么意义的，所以如果预测为 IDLE，则转为防御。
毕竟，人类新手玩家打弦一郎的时候，也极少会 IDLE 啥也不做，更多时候是在防御状态。

另：有两种情况，AI 并不会处理：

其一，当游戏角色躺在地上的时候，AI 不会按垫步键快速起身，需要玩家来处理一下。

其二，喝血瓶也是一个很难的事情，包括一些人类玩家，都找不好喝血瓶的时机，AI也没有做这方面的专门训练，需要玩家按 ] 键暂停预测，找机会喝个血瓶，然后再次按下 ] 按恢复预测。

这主要是因为 AI 在训练的时候，只关注了敌兵的姿势，没有关注自己的 HP 与 姿势。

## 人工备份

模型的训练结果主要涉及到如下的几个文件：

- images	其中 original 为收集到的截屏图像文件，train/test 为划分好的训练集与测试集，也就是数据集中的 X 

- labels.csv  数据集中的 y ，记录了每一个图像文件对应的 label(y) 值。

- model.resnet.v1  训练好的模型


如果要训练新模型的话，可能需要对老模型的这些数据进行备份。


## 大部分代码和思路来自以下仓库，感谢他们

- https://github.com/XR-stb/DQN_WUKONG
- https://github.com/analoganddigital/DQN_play_sekiro
- https://github.com/Sentdex/pygta5
- https://github.com/RongKaiWeskerMA/sekiro_play
- https://www.lapis.cafe/posts/ai-and-deep-learning/%E4%BD%BF%E7%94%A8resnet%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B

